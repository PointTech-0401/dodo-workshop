# FastMCP Agent - MCP Client å¯¦ä½œ

import json
import asyncio
from fastmcp import Client
from llm.client import chat_completion
from agent.prompt import SYSTEM_PROMPT
from memory.local_memory import LocalMemory


class FastMCPAgent:
    def __init__(self):
        self.memory = LocalMemory()
        self.history = []
        self.client = None
        self.tools_cache = []
    
    async def connect(self, server: str):
        """é€£æ¥åˆ° FastMCP Server
        
        Args:
            server: MCP Server ä½ç½®ï¼Œæ”¯æ´ä»¥ä¸‹æ ¼å¼ï¼š
                - æœ¬åœ°è…³æœ¬: "mcp_server/tools_server.py"
                - SSE é€£ç·š: "http://192.168.1.168:8234/sse"
                - Streamable HTTP: "http://192.168.1.168:8234/mcp"
        """
        self.client = Client(server)
        await self.client.__aenter__()
        
        # å–å¾—å·¥å…·æ¸…å–®ä¸¦è½‰æ›ç‚º OpenAI æ ¼å¼
        tools_result = await self.client.list_tools()
        self.tools_cache = [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            }
            for tool in tools_result
        ]
        print(f"âœ… å·²é€£æ¥ MCP Server: {server}")
        print(f"   è¼‰å…¥ {len(self.tools_cache)} å€‹å·¥å…·")
        for tool in tools_result:
            print(f"   - {tool.name}: {tool.description[:50]}...")
    
    def get_memory_content(self):
        """å–å¾—è¨˜æ†¶å…§å®¹"""
        memory_content = self.memory.load()
        return f"Memory:\n{memory_content}"

    async def chat(self, user_input: str) -> str:
        """èˆ‡ Agent å°è©±
        
        Args:
            user_input: ä½¿ç”¨è€…è¼¸å…¥
            
        Returns:
            Agent çš„å›æ‡‰
        """
        self.history.append({"role": "user", "content": user_input})

        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "system", "content": self.get_memory_content()}
        ] + self.history

        # å‘¼å« LLMï¼Œå¸¶å…¥ MCP å·¥å…·
        message = chat_completion(messages, tools=self.tools_cache)

        # å¦‚æœ LLM æ±ºå®šå‘¼å«å·¥å…·
        if message.tool_calls:
            tool_call = message.tool_calls[0]
            tool_name = tool_call.function.name
            args = json.loads(tool_call.function.arguments)
            
            print(f"ğŸ”§ å‘¼å« MCP å·¥å…·: {tool_name}")
            print(f"   åƒæ•¸: {args}")
            
            # é€é MCP Client å‘¼å«å·¥å…·
            result = await self.client.call_tool(tool_name, args)
            
            # å–å¾—å·¥å…·å›å‚³çš„æ–‡å­—å…§å®¹
            # FastMCP å›å‚³ CallToolResult ç‰©ä»¶ï¼Œå…§å®¹åœ¨ .content å±¬æ€§ä¸­
            if result and result.content:
                result_text = result.content[0].text
            else:
                result_text = "ç„¡çµæœ"
            print(f"   çµæœ: {result_text}")

            # å°‡å·¥å…·å‘¼å«å’ŒçµæœåŠ å…¥æ­·å²
            self.history.append(message)
            self.history.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": result_text
            })

            # é‡æ–°è¼‰å…¥è¨˜æ†¶ä¸¦è®“ LLM ç”Ÿæˆæœ€çµ‚å›æ‡‰
            messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "system", "content": self.get_memory_content()}
            ] + self.history

            final = chat_completion(messages)
            self.history.append(final)
            return final.content

        # æ²’æœ‰å·¥å…·å‘¼å«ï¼Œç›´æ¥å›å‚³
        self.history.append(message)
        return message.content
    
    async def close(self):
        """é—œé–‰ MCP é€£ç·š"""
        if self.client:
            await self.client.__aexit__(None, None, None)
            print("ğŸ‘‹ å·²é—œé–‰ MCP é€£ç·š")
